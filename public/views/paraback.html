<!DOCTYPE html>
<html lang="en">

<head>
    <title>adam optimizer - visNLP</title>
    <link rel="stylesheet" href="../style.css" />
</head>

<body>
    <div id="navbar">
        <button class="navelement" id="homenav" onclick="window.location='/'">
          <img src="assets/wpi_logo_transparent_white.png" alt="wpi logo2" class="nav-button-icon">
          <h1>visNLP 2.0 Home</h1>
        </button>
        <button class="navelement"        onclick="window.location='/w2v'"       ><h1>Word2Vec</h1></button>
        <button class="navelement"        onclick="window.location='/senback'"   ><h1>Sen2Vec</h1></button>
        <button class="navelement-curr"   onclick="window.location='/paraback'"  ><h1>Para2Vec</h1></button>
        <button class="navelement"        onclick="window.location='/adam'"      ><h1>Adam Optimizer</h1></button>
        <button class="navelement"        onclick="window.location='/songback'"  ><h1>Songs Recs App</h1></button>
        <button class="navelement"        onclick="window.location='/newsback'"  ><h1>Fake News App</h1></button>
    </div>

    <div id="sidebar_adam">
        <button id="switch_adam_button" onclick="window.location='/p2v'">
            <h1>Para2Vec Step-by-Step Simulation</h1>
        </button>
        
        <div style="height:6vh;"></div>
        <div class="sidebar-web-diagram">
            <img src="/assets/web_diagrams/web_diagram_condensed_para2vec.png" alt="adam web diagram">
        </div>
        <div style="height:8vh;"></div>

        <button class="prev-topic-button" onclick="window.location='/senback'">
            <h2>Prev:</h2>
            <h1>Sen2Vec</h1>
        </button>
        <button class="next-topic-button" onclick="window.location='/adam'">
            <h2>Next:</h2>
            <h1>Adam Optim.</h1>
        </button>
    </div>

    <div id="restofpage_overview">
        <!-- title -->
        <h1> Paragraph2Vec </h1>

        <!-- content sec | TOPIC RUNDOWN -->
        <h2> Introduction to the Para2vec Algoritm </h2>

        <p> Para2Vec is a natural language processing (NLP) algorithm that is widely used for document clustering, topic 
            modeling, and information retrieval tasks. This algorithm is an extension of the popular Word2Vec algorithm 
            and allows you to learn vector representations of entire documents or paragraphs, rather than just individual 
            words. By doing so, Para2Vec helps to capture the semantic meaning of entire documents, which can be very useful 
            in many applications. </p>

        <p> One of the main advantages of Para2Vec is that it allows you to work with large volumes of text data, without 
            losing the semantic meaning of the documents. This algorithm can be used for a variety of NLP tasks, including 
            text classification, information retrieval, and clustering. With the increasing importance of natural language 
            processing in many industries, Para2Vec is becoming an essential tool for data scientists and NLP practitioners. </p>

        <!-- content sec | HOW IT WORKS -->

        <h2> How the Para2vec Algorithm Works </h2>

        <p> The Para2Vec algorithm is based on a neural network architecture that consists of an input layer, a hidden 
            layer, and an output layer. In this architecture, each paragraph or document is represented as a vector in 
            the input layer. The hidden layer is responsible for capturing the semantic meaning of the input vectors, 
            while the output layer is used to predict the context of the input vectors. During the training process, 
            Para2Vec adjusts the weights of the neural network to minimize the prediction error between the predicted 
            context and the actual context. </p>

        <p> The Process of Performing Para2vec: </p>

        <ul>
            <li>Input Data is fed into sen2vec algorithm </li>
            <li>Creates paragraph IDs and vocabulary, initializing word vectors</li>
            <li>Input matrix is created based on the paragraph ID, context words, and center words </li>
            <li>Word and paragraph weight matrices are initialized along with a bias matrix</li>
            <li>The predicted center word matrix is computed and softmax is applied</li>
            <li>Negative log likelihood loss is calculated using both the predicted center word matrix and the true word matrix</li>
            <li>The adam optimizer is then used to update the word and paragraph weight matrices along with the bias matrices</li>
            <li>The corresponding updated matrices are used in the next epoch</li>
            <li>This process is repeated until optimization is met, resulting in the final paragraph vectors</li>
        </ul>

        <p> The step-by-step simulation provides a more detailed explanation of the individual components of this algorithm. </p>

    </div>
</body>

</html>